\documentclass[a4paper,12pt]{article}

\usepackage{amsfonts, amsmath, amssymb, authblk, scrextend, hyperref, enumerate,mathtools, tikz, csquotes, mathrsfs, lmodern,arydshln, xypic, bbold, graphicx}
\usepackage[mathcal]{eucal}
\usetikzlibrary{matrix}
\usepackage[lmargin=2.5 cm,rmargin=2.5cm,tmargin=3cm,bmargin=3cm]{geometry}

\allowdisplaybreaks
\everymath{\displaystyle}
\setlength\parindent{0pt}


\newcommand*\encircled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}
\newcommand*\rected[1]{\tikz[baseline=(char.base)]{
            \node[shape=rectangle,draw,inner sep=2pt] (char) {#1};}}
\newcommand{\dlim}{\underset{\longrightarrow}{\lim} \ }
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\one}{\field{1}}
\newcommand{\disp}{\displaystyle}
\newcommand{\Z}{\field{Z}}
\newcommand{\Q}{\field{Q}}
\newcommand{\F}{\field{F}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\renewcommand{\det}[1]{\text{det}\3(#1\4)}
\newcommand{\ind}{\mathbb{1}}
\newcommand{\var}{\text{Var}}
\newcommand{\val}{\text{Val}}
\newcommand{\sd}{\text{SD}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\pr}{\text{Pr}}
\renewcommand{\bf}[1]{\textbf{#1}}
\renewcommand{\it}[1]{\textit{#1}}
\renewcommand{\tt}[1]{\texttt{#1}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\mscr}[1]{\mathscr{#1}}
\newcommand{\spec}{\textbf{spec}}
\newcommand{\A}{\field{A}}
\newcommand{\liff}{\leftrightarrow}
\newcommand{\tr}[1]{\text{trace}\3(#1\4)}
\newcommand{\limf}{\lim_{n \to \infty}}
\newcommand{\3}{\left}
\newcommand{\4}{\right}
\renewcommand{\-}[1]{{}^{-#1}}
\newcommand{\up}[1]{{}^{#1}}
\newcommand{\Id}{\text{Id}}
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\bdiv}{\ \textbf{div} \ }
\newcommand{\bbmod}{\ \textbf{mod} \ }
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Rel}{\text{Rel}}
\newcommand{\id}[1]{\text{id}_{#1}}
\newcommand{\proj}[1]{\text{proj}_{#1}}
\newcommand{\ppmod}[1]{\ (\text{mod $#1$})}
\newcommand{\ceil}[1]{\3\lceil \text{$#1$} \4\rceil}
\newcommand{\floor}[1]{\3\lfloor \text{$#1$} \4\rfloor}
\newcommand{\power}[1]{\mathcal{P}(#1)}
\newcommand{\tri}[1]{\triangle_{#1}}
\newcommand{\inj}{\hookrightarrow}
\newcommand{\im}{\text{Im}}
\newcommand{\edel}{\epsilon-\delta}
\newcommand{\empt}{ \varnothing}
\newcommand{\inv}{^{-1}}
\newcommand{\ideal}[1]{\mathfrak{#1}}
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}
\begin{document}
\title{\vspace{-1.5 cm}\bf{ \Large PredictionIO Engine Template: Modeling Text Data}}
\author{\vspace{-1.5cm}}
\date{}
\maketitle

\section*{\normalsize Introduction}

The purpose of this tutorial is to illustrate how to model text data using PredictionIO's engine platform. We will be largely mirroring the tutorial provided for Scikit's Learn package title \enquote{Working With Text Data.} However, we will be providing instructions for building the model using PredictionIO's engine platform. The advantages of using this framework include the usage of Apache Spark for distributed computation which is crucial for computation with large data sets, as well as the capacity to use a newly trained predictive model to respond to queries in real-time. 

We will assume the user is running the PredictionIO version 0.9.2, and meets all minimum computing requirements. To download PredictionIO, follow the instructions on the Getting Started tutorial.

\section*{\normalsize Engine Overview}




As a user, we are charged with collecting data and importing it into an event server. The data is then read and processed by our engine via the \tt{DataSource} and \tt{Preparation} components. The \tt{Algorithm} engine component then trains a predictive model using the processed, or prepared, data. Once we have trained a model, we are ready to deploy our engine and respond to real-time queries via the \tt{Serving} component. 

We are given our observations as a set of strings $\mathcal{S}.$ In a general setting, there are two different kind of problems we can be given. In the first, each string $s$ contained in $\mathcal{S}$ is also associated to a class label $y$ taking values in $\{1, 2, ..., K\},$ where $K$ is an integer, which we can think of as a categorical, or class, label.We may proceed by learning a predictive model $f,$ a function which takes in a string $s$ and outputs a class label, using the observed data pairs $(y, s).$ Once we have learned $f$ then given a new string $s_\text{new},$ we can predict a class label $f(s_\text{new}).$ This is an example of a supervised learning problem. To exemplify the second kind of problem, suppose we are only given the set of strings $\mathcal {S},$ and are asked to group the string together into $K$ different groups based on a set of characteristics which we extract from the strings. This is an example of an unsupervised learning problem. 

In both cases, we are tasked with learning a model $f.$ However, before this we must first represent our strings $s$ in such a manner so that we may apply existing machine learning methods provided in the Spark MLLib library. We will be using the Apache OpenNLP tools library for the purpose of data representation. 

Our engine includes a Data Model implementation which deals with representing our text data as numeric feature vectors. The Training Model here is an abstract framework representing a set of Scala classes that can produce a supervised (or unsupervised) learning model. For our supervised training model implementation, $f$ will be a classifier, and for our unsupervised training model, an assignment of class labels for each string in $\mathcal{S}.$ The following diagram shows the how the provided Modeling Text Data Engine template functionality is structured, and the interactions between your web/app, the provided Event Server, and our Modeling Text Data Engine.




\centerline{
\xymatrixcolsep{3pc}\xymatrix{
& \\
\encircled{\bf{Event Server}} \ar[r] & \\
& \ar[ldd] \\
& \\
\encircled{\bf{Your Web/App}} \ar[ruu] \ar[uuu]^{\text{Event Data}}& 
}
\rected{
\xymatrixcolsep{2pc}\xymatrix{
\encircled{Data Source}\ar[r]& \encircled{Preparator} \ar[r] \ar[d]& \encircled{Algorithm} \ar[d] \ar[r] & \encircled{Serving} \\
\bf{Engine} & \encircled{Data Model} \ar[ru] & \encircled{Training Model} \ar[ru]&
}}}


We begin the data collection stage by importing a Scikit Learn dataset, \tt{20newsgroups}, into PredictionIO's event server. 

\section*{\normalsize Importing Data}

We will be using Scikit Learn's datasets module. Let's get a better feel for the data: initialize your Python interpreter and type the following lines:

\begin{verbatim}
>>> from sklearn.datasets import fetch_20newsgroups
>>> twenty_train = fetch_20newsgroups(subset = 'train',
...                                     shuffle = True,
...                                     random_state = 10)
\end{verbatim}

This initializes a dictionary-like object as can be seen in the following output:

\begin{verbatim}
>>> twenty_train.keys()
dict_keys(['filenames', 'DESCR', 'target', 'data', 'target_names'])
\end{verbatim}

What we are interested in here are the target an data entries. Type in the following line to see how we prepare the Scikit data prior to importing it to the PredictionIO Event Server.

\begin{verbatim}
>>> [(twenty_train.target[k], twenty_train.data[k]) 
...  for k in range(len(twenty_train.data))][0]
\end{verbatim}

Our main tool from the Python SDK that we will use to import our data into the event server is the class \tt{EventClient} and its method \tt{create\_event}. Now, in your data directory create a file named \tt{import\_eventserver.py}. Initialize the script by adding the following lines:
\begin{verbatim}
import predictionio
import argparse
\end{verbatim}
We will now define a function \tt{import\_events} which will take as input an object of type \tt{EventClient} and a string object providing the name of the data document, which is in our case \tt{train.tsv}. We will skip the details of the document processing in Python, and refer to the reader to the following documentation:
$$
\tt{https://docs.python.org/2/library/strings.html}
$$
We will process each line in the file and convert it to a list of strings. Once each line is processed, an event is created using the aforementioned client method. We will be assigning various properties to our data so that our engine can successfully read the corresponding event data parts that will be needed to train our predictive model. Your function should look similar to the following:
\begin{verbatim}
def import_events(client):
    train = ((float(twenty_train.target[k]), twenty_train.data[k])
             for k in range(len(twenty_train.data)))
    count = 0
    print('Importing data.....')
    for elem in train:
        count += 1
        client.create_event(
            event = "documents",
            entity_id = count,
            entity_type = "source",
            properties = {
                "label": elem[0],
                "text": elem[1]
            })
    print("Imported {0} events.".format(count))
\end{verbatim}




\end{document}