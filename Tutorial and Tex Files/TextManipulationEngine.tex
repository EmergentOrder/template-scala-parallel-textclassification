\documentclass[a4paper,12pt]{article}

\usepackage{amsfonts, amsmath, amssymb, authblk, scrextend, hyperref, enumerate,mathtools, tikz, csquotes, mathrsfs, lmodern,arydshln, xypic, bbold, graphicx, setspace}
\usepackage[mathcal]{eucal}
\usetikzlibrary{matrix}
\usepackage[lmargin=2.5 cm,rmargin=2.5cm,tmargin=3cm,bmargin=3cm]{geometry}

\allowdisplaybreaks
\everymath{\displaystyle}
\setlength\parindent{0pt}
\setlength\parskip{7.5pt}

\input xy
\xyoption{all}


\newcommand*\encircled[1]{\tikz[baseline=(char.base)]{
            \node[black,fill=cyan,shape=circle,draw,inner sep=1pt] (char) {#1};}}
\newcommand*\encircledblack[1]{\tikz[baseline=(char.base)]{
            \node[black,fill=cyan,shape=circle,draw,inner sep=1pt] (char) {#1};}}
\newcommand*\rected[1]{\tikz[baseline=(char.base)]{
            \node[black,fill=gray,shape=rectangle,draw,inner sep=2pt,] (char) {#1};}}
\newcommand*\rectedblack[1]{\tikz[baseline=(char.base)]{
            \node[gray,fill=black,shape=rectangle,draw,inner sep=2pt,] (char) {#1};}}
\newcommand{\dlim}{\underset{\longrightarrow}{\lim} \ }
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\one}{\field{1}}
\newcommand{\disp}{\displaystyle}
\newcommand{\Z}{\field{Z}}
\newcommand{\Q}{\field{Q}}
\newcommand{\F}{\field{F}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\renewcommand{\det}[1]{\text{det}\3(#1\4)}
\newcommand{\ind}{\mathbb{1}}
\newcommand{\var}{\text{Var}}
\newcommand{\val}{\text{Val}}
\newcommand{\sd}{\text{SD}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\pr}{\text{Pr}}
\renewcommand{\bf}[1]{\textbf{#1}}
\renewcommand{\it}[1]{\textit{#1}}
\renewcommand{\tt}[1]{\texttt{#1}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\mscr}[1]{\mathscr{#1}}
\newcommand{\spec}{\textbf{spec}}
\newcommand{\A}{\field{A}}
\newcommand{\liff}{\leftrightarrow}
\newcommand{\tr}[1]{\text{trace}\3(#1\4)}
\newcommand{\limf}{\lim_{n \to \infty}}
\newcommand{\3}{\left}
\newcommand{\4}{\right}
\renewcommand{\-}[1]{{}^{-#1}}
\newcommand{\up}[1]{{}^{#1}}
\newcommand{\Id}{\text{Id}}
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\bdiv}{\ \textbf{div} \ }
\newcommand{\bbmod}{\ \textbf{mod} \ }
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Rel}{\text{Rel}}
\newcommand{\id}[1]{\text{id}_{#1}}
\newcommand{\proj}[1]{\text{proj}_{#1}}
\newcommand{\ppmod}[1]{\ (\text{mod $#1$})}
\newcommand{\ceil}[1]{\3\lceil \text{$#1$} \4\rceil}
\newcommand{\floor}[1]{\3\lfloor \text{$#1$} \4\rfloor}
\newcommand{\power}[1]{\mathcal{P}(#1)}
\newcommand{\tri}[1]{\triangle_{#1}}
\newcommand{\inj}{\hookrightarrow}
\newcommand{\im}{\text{Im}}
\newcommand{\edel}{\epsilon-\delta}
\newcommand{\empt}{ \varnothing}
\newcommand{\inv}{^{-1}}
\newcommand{\ideal}[1]{\mathfrak{#1}}
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}
\begin{document}
\title{\vspace{-1.5 cm}\bf{PredictionIO: Text Manipulation Engine}}
\author{\vspace{-2cm}}
\date{}
\maketitle


In the real world, there are many applications that collect text as data. For example, suppose that you have a set of news articles that are categorized based on content, and you wish to automatically assign incoming, uncategorized articles to one of the existing categories. There are a wide array of machine learning models you can use create, or train, a predictive model to assign an incoming article, or query, to an existing category. Before you can use these techniques you must first transform the text data (in this case the set of news articles) into numeric vectors, or feature vectors, that can be used to train your model.

The purpose of this tutorial is to illustrate how you can go about doing this using PredictionIO's platform. The advantages of using this platform include: a dynamic engine that responds to queries in real-time; separation of concerns,
%http://en.wikipedia.org/wiki/Separation_of_concerns
 which offers code re-use and maintainability, and distributed computing capabilities for scalability and efficiency. Moreover, it is easy to incorporate non-trivial data modeling tasks into the DASE architecture allowing Data Scientists to focus on tasks related to modeling. We will exemplify these ideas in this tutorial, and, in particular, show you how to:

\begin{itemize}
\item[$\bullet$]{import a corpus of text documents into PredictionIO's event server;}

\item[$\bullet$]{read the imported event data for use in text processing;}

\item[$\bullet$]{transform document text into a feature vector (we will be modeling each document using n-grams and the tf-idf transformation);}

\item[$\bullet$]{use the feature vectors to fit a classification model based on Naive Bayes (using Spark MLLib library implementation);}

\item[$\bullet$]{use the feature vectors to fit a classification model based on Latent Dirichlet Allocation (using Spark MLLib library implementation).}

\item[$\bullet$]{evaluate the performance of the fitted models;}

\item[$\bullet$]{yield predictions to queries in real-time using a fitted model.}
\end{itemize}


\section*{Prerequisites}

Before getting started, please make sure that you have the latest version of PredictionIO installed 
%https://docs.prediction.io/install/
. You will also need PredictionIO's Python SDK 
%https://github.com/PredictionIO/PredictionIO-Python-SDK
, and the Scikit learn library (http://scikit-learn.org/stable/) for importing a sample data set into the PredictionIO Event Server. Any Python version greater than 2.7 will work for the purposes of executing the \tt{data/import\_eventserver.py} script provided with this engine template. Moreover, we emphasize here that this is an engine template written in \it{Scala} and can be more generally thought of as an SBT project containing all the necessary components.

You should also download the engine template named Modeling Text Data 
%http://templates.prediction.io/
 that accompanies this tutorial.

\section*{Engine Overview}

The engine follows the DASE architecture which we briefly review here. As a user, you are tasked with collecting data for your web or application, and importing it into PredictionIO's Event Server. Once the data is in the server, it  can be read and processed by the engine via the Data Source and Preparation components, respectively. The Algorithm component then trains a predictive model using the processed, or prepared, data. Once we have trained a model, we are ready to deploy our engine and respond to real-time queries via the Serving component. The Evaluation component is used to compute an appropriate metric to test the performance of a fitted model, as well as aid in the tuning of model hyper parameters. 

We are working with text data which means our queries, or newly observed documents, are of the form 
$$
\tt{\{text : String\}}.
$$
In our example, a query would be an incoming news article. Once the engine is deployed it can process the query, and then return a Predicted Result of the form 
$$
\tt{\{category : String, confidence : Double\}}.
$$
Here category is the model's class assignment for this new text document (i.e. our best guess for this article's categorization), and confidence, a value between 0 and 1 representing our confidence in the category prediction (0 meaning we have no confidence in the prediction). The Actual Result is of the form 
$$
\tt{\{category : String\}}.
$$
This is used in the evaluation stage when estimating the performance of our predictive model (how well does the model predict categories). 
 
In addition to the DASE components, our engine also includes the Data Model and Training Model abstractions. The Data Model refers to the implementation of modeling choices relating to feature extraction and selection. In our example, this includes the vectorization of text and any subsequent feature reduction or transformation procedures one might want to do. The Training Model abstraction refers to any set of Scala classes that take in a set of feature observations and outputs a predictive model. In our example, this includes the NBModel (Naive Bayes) and LDAModel (Latent Dirichlet Allocation) based classes which give us category classification models.

To summarize: (1) we import text data to the Event Server; (2) the engine reads and processes the data, and trains a predictive model; (3) once the model is trained and the engine deployed, your web application can send text queries to which the engine can respond with a predicted result in real time; (4) in the evaluation, the engine produces both predicted results and actual results for the training data and feeds them to the Evaluation component. The figure below shows a graphical representation of the engine architecture just described, as well as its interactions with your web/app and a provided Event Server:

\vspace{0.05cm}
\centerline{
\rectedblack{
\xymatrixcolsep{3pc}\xymatrix{
\encircled{\bf{Evaluation}} & \ar[l]_(0.25){\text{Predicted Result}}^(0.25){\text{Actual Result}} \\
\encircled{\bf{Event Server}} \ar[r]^(0.7){\text{Event}} & \\
& \ar@<1ex>[ld]^(0.3){\text{Predicted Result}} \\
\encircled{\bf{Your Web/App}} \ar@<1ex>[ru]^(0.7){\text{Query}} \ar[uu]^{\text{Text Data}}& 
}
\hspace{-1cm}
\rected{
\xymatrixcolsep{2pc}\xymatrix{
\encircledblack{Data Source}\ar[r]& \encircledblack{Preparator} \ar[r] & \encircledblack{Algorithm}  \ar[r] & \encircledblack{Serving} \\
\bf{Engine} & \encircledblack{Data Model} \ar[u] & \encircledblack{Training Model} \ar[u]&
}}}}



\section*{Quick Start}

This is a quick start guide in case you want to start using the engine right away. For more detailed information, read the subsequent sections.

\begin{itemize}
\item[1.]{Create a new application. After the application is created, you will be given an access key for the application.
\begin{verbatim}
$ pio app new MyTextApp
\end{verbatim}}

\item[2.]{Import the tutorial data, and be sure to replace \tt{***} with the access key obtained from the latter step.
\begin{verbatim}
$ python import_eventserver.py --access_key ***
\end{verbatim}}

\item[3.]{Set the engine parameters in the file \tt{engine.json}. The default settings are shown below.

\break

\begin{verbatim}
{
  "id": "default",
  "description": "Default settings",
  "engineFactory": "TextManipulationEngine.TextManipulationEngine",
  "datasource": {
    "params": {
      "appName": "marco-testapp",
      "evalK": 5
    }
  },
  "preparator": {
    "params": {
      "nMin": 1,
      "nMax": 2
    }
  },
  "algorithms": [
    {
      "name": "sup",
      "params": {
        "lambda": 0.5
      }
    }
  ]
}
\end{verbatim}}

\item[4.]{Build your engine.
\begin{verbatim}
$ pio build
\end{verbatim}}

\item[5.a.]{Evaluate your training model and tune parameters.
\begin{verbatim}
$ pio eval
\end{verbatim}}

\item[5.a.]{Train your model and deploy.
\begin{verbatim}
$ pio train
$ pio deploy
\end{verbatim}}

\end{itemize}

Depending on your needs, in steps (5.x.) above, you can configure your Spark settings by typing a command of the form:
\begin{verbatim}
$ pio command -- --master url --driver-memory {0}G --executor-memory {1}G 
	--conf spark.akka.framesize={2} --total_executor_cores {3}
\end{verbatim}
We only list the latter commands as these are some of the more commonly modified values. See the Spark documentation
%https://spark.apache.org/docs/latest/spark-standalone.html
and the PredictionIO FAQ's 
%https://docs.prediction.io/resources/faq/
for more information


\section*{Importing Data}

In order to stick with the news article example, we will, for the purpose of this illustration, be importing two different sources of data from the Scikit learn Python library into PredictionIO's Event Server: a corpus of text documents that are categorized into a set of topics, as well as a set of stop words. Stop words are words that we do not want to include in our corpus when modeling our text data. 

For the remainder of the tutorial, we will assume that the present working directory is the engine template root directory. The script used to import the data is \tt{import\_eventserver.py} located in the data directory. To actually import the data into our Event Server, we must first create an application which we will name MyTextApp. To do this run the shell command \tt{pio app new MyTextApp}, and take note of your access key. If you forget your access key, you can obtain it by using the command \tt{pio app list}. The following shell output shows the command needed for importing your data (first line), and the resulting output after the data has been successfully imported. Replace \tt{***} with your actual access key, and \tt{???} with the correct location of the port hosting HBase. You can usually leave out the url field as HBase will generally be hosting on port 7070.

\begin{verbatim}
$ python data/import_eventserver.py --access_key *** --url ???
Importing data.....
Imported 11314 events.
Importing stop words.....
Imported 318 stop words.
 \end{verbatim}

\section*{Data Source: Reading Event Data}

Now that our data has been imported into PredictionIO's Event Server, it needs to be read from HBase for it to actually be used by our engine. This is precisely what the DataSource engine component is for. We first explain the classes Observation and Training Data which are defined in \tt{DataSource.scala}. Observation serves as a wrapper for storing the information about a news document needed to train a model. The class member label refers to the label of the category a document belongs to, and text, stores the actual document content. TrainingData is used to store an RDD of Observation objects and our set of stop words. 

The class DataSourceParams is used to specify the parameters needed to read and prepare the data for processing. This class is initialized with two parameters appName and evalK. The first parameter specifies your application name (i.e. MyTextApp), which is needed so that the DataSource component knows where to pull the event data from. The second parameter is used for model evaluation and specifies the number of folds to use in cross-validation 
%http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29
when we estimate a model performance metric.

\break

Finally, we come to the DataSource class. This is initialized with its corresponding parameter class, and extends PDataSource[TD, E, Q, AR]. This extension means that it \bf{must} implement the method readTraining which returns an instance of type TD which is in this case the class TrainingData. This method completely relies on the defined \it{private} methods readEventData and readStopWords. Both of these functions read data observations as Event instances, create an RDD containing these events and finally transforms the RDD of events into an object of the appropriate type as seen below:

\begin{verbatim}
private def readEventData(sc: SparkContext) : RDD[Observation] = {
    //Get RDD of Events.
    PEventStore.find(
      appName = dsp.appName,
      entityType = Some("source"), // specify data entity type
      eventNames = Some(List("documents")) // specify data event name

      // Convert collected RDD of events to and RDD of Observation
      // objects.
    )(sc).map(e => Observation(
      e.properties.get[Double]("label"),
      e.properties.get[String]("text")
    )).cache
  }

  // Helper function used to store stop words from
  // event server.
  private def readStopWords(sc : SparkContext) : Set[String] = {
    PEventStore.find(
      appName = dsp.appName,
      entityType = Some("resource"),
      eventNames = Some(List("stopwords"))

    //Convert collected RDD of strings to a string set.
    )(sc)
      .map(e => e.properties.get[String]("word"))
      .collect
      .toSet
  }
\end{verbatim}

Note that readEventData and readStopWords use different entity types and event names, but use the same application name. This is because we imported both our corpus and stop word set using the same access key. These field distinctions are required for distinguishing between the two data types. Also note that these methods require a SparkContext to be passed as a parameter. The method readEval also relies on readEventData and readStopWords, and its function is to prepare the different cross-validation folds needed for evaluating your model and tuning hyper parameters. 

\section*{Processing the Data}

This section deal with the aspect of transforming the data you read in the previous stage into something that you can actually use to train a predictive model. As you will see, most of the work in the processing takes place in the Data Model stage. This decoupling allows you to focus primarily on development issues regarding your model, and also allows Data Scientists an easy way to incorporate their modeling ideas into the DASE framework. 

\subsection*{Data Model : Data Representation}

For our Text Manipulation Engine, the Data Model abstraction is implemented as a Scala class taking in the parameters td, nMin, nMax, where td is an object of class TrainingData. The other two parameters are the components of our n-gram window which we will define shortly. In this section, we cover the default feature preparation procedure implemented in the engine template. It will be easier to explain this process with an example, so consider the document:
$$
D := \tt{"Hello, my name is Marco."}
$$
The first thing we need to do is break up $D$ into an array \enquote{allowed tokens.} You can think of a token as a terminating sequence of characters that exist in our document (think of a word in a sentence). For example, the list of tokens that appear in $D$ is:
$$
\tt{val A = Array("Hello", ",", "my",  "name", "is", "Marco", ".")}
$$
Now, recall that when we imported our data, we also imported a set of stop words. This set of stop words contains all the words (or tokens) that we do not want to include once we tokenize our documents. Hence, we will call the tokens that appear in $D$ and are not contained in our set of stop words allowed tokens. So, if our set of stop words is $\{\tt{"my"}, \tt{"is"}\},$ then the list of allowed tokens appearing in $D$ is:
$$
\tt{val A = Array("Hello", ",",  "name", "Marco", ".")}
$$
We call any function that takes as input a text document and returns an array of allowed tokens a tokenizer. The tokenizer in our Data Model implementation is the private method tokenize. We use the SimpleTokenizer from the OpenNLP library to implement this (note that you must add the Maven dependency declaration to your \tt{build.sbt} file to incorporate this library into your engine).

 The next step in the data representation is to take the array of allowed tokens and extract a set of n-grams and a corresponding value indicating the number of times a given n-gram appears. The set of n-grams for n equal to 1 and 2 in the running example is the set of elements of the form \tt{[A($i$)]} and \tt{[A($j$), A($j + 1$)]}, respectively. The n-gram window is an interval of integers for which we extract grams for each element. nMin and nMax are the smallest and largest integer values in the interval, respectively, for $i = 0, 1, ..., n - 1,$ and $j = 0, ..., n - 2.$ 

In our implementation, the n-gram extraction and counting procedure is carried out by the private method hash, which returns a Map with keys, n-grams, and values, the number of times each n-gram is extracted from the document. We use OpenNLP's NGramModel class to extract n-grams.

The next step is, once all of the observations have been hashed, to collect all n-grams and compute their corresponding t.f.-i.d.f. value. The t.f.-i.d.f. transformation is a transformation that intuitively helps to give less weight to those n-grams that appear with high frequency across all documents, and vice versa. This helps to leverage the predictive power of those words that appear rarely, but can make a big difference in the categorization of a given article. In our implementation, the private method createUniverse outputs an RDD of pairs, where an n-gram is matched with it's i.d.f. value. This RDD is collected as a HashMap (this will be used in future RDD computations so we need an object that is serializable), and we create a second hash map with n-grams associated to indices. This gives a global index to each n-gram so that each document observation is vectorized in the same manner. 

The last two functions we mention are the methods you will actually use for the data transformation. The method transform takes a document and outputs a sparse vector (MLLib implementation). The transformData simply transforms the TrainingData input (a corpus of documents) into a set of vectors that can now be used for training. The method transform is used both to transform the training data and future queries. It is important to note that using all 11,314 news article observations without any pre-processing of the documents results in over 1 million unigram and bigram features, so that a sparse vector representation is necessary to save some serious computation time.


\subsection*{Preparator : Data Processing in DASE}

Recall that the Preparator stage is used for doing any prior data processing needed to fit a predictive model. In line with the separation of concerns, the Data Model implementation is built to do the heavy lifting needed for this data processing. Our Preparator can now simply implement the prepare method which outputs an object of type PreparedData, which in our case serves as a wrapper for an initialized DataModel object. Since our DataModel class requires us to specify the two n-gram window components, we must incorporate the custom class of parameters for our Preparator component, PreparatorParams. 

The simplicity of this stage implementation truly exemplifies one of the benefits of using the PredictionIO platform. For developers, it is easy to incorporate different classes and tools into the DASE framework so that the process of creating an engine is greatly simplified which helps increase your productivity. For data scientists, the load of implementation details you need to worry about is minimized so that you can focus on what is important to you, training a good predictive model. 

\section*{Training The Model}

This section will guide you through the two Training Model implementations that come with this engine template. Recall that the Training Model abstraction refers to an arbitrary set Scala Class that outputs a predictive model (i.e. implements some method that can be used for prediction). The general problem this engine template is tackling is text classification, so that our Training Model abstraction domain is restricted to implementations producing classifiers. In particular, the two classification models that are implemented in this engine template are based on Multinomial Naive Bayes and Latent Dirichlet Allocation using t.f.-i.d.f. vectorized text. 

\subsection*{Naive Bayes Classification}

We will be using the Multinomial Naive Bayes implementation found in the Spark MLLib library. However, recall that the predicted results required in the specs listed in our overview must be of the form.
$$
\tt{\{category: String, confidence: Double\}}.
$$
We interpret here the confidence value as the probability that a document belongs to a category given the vectorized data. Now, note that MLLib's Naive Bayes model has the class members pi ($\pi$) and theta($\theta$). $\pi$ is a vector of log prior class probabilities, and $\theta$ is a CxD matrix, where C is the number of classes and D, the number of features, giving the log probabilities that parametrize a Multinomial model. Say we are given a document and vectorize it. Letting \bf{x} denote this vectorized form, then it can be shown that the vector 
$$
\frac{\exp\3(\pi + \theta\bf{x}\4)}{||\exp\3(\pi + \theta\bf{x}\4)||}
$$
is a vector with C components that represent the posterior class probabilities given \bf{x}. This is essentially the motivation behind defining the class NBModel, which initially uses MLLib's NaiveBayesModel in the implementation. The private methods innerProduct and getScores are implemented to do the computation above. Once we have the vector of class probabilities, we categorize the text document to the class with highest posterior probability and both the category as well as the probability of belonging to that category (i.e. our confidence in the prediction). This is implemented in the method predict.

\subsection*{LDA Based Classification}

\subsection*{Algorithm Component}

Each of our training model classes is accompanied by a corresponding algorithm component. For example, NBModel accompanies NBAlgorithm, and LDAModel, LDAAlgorithm. The implementation details are largely simplified by our Training Model abstraction, since we take care of the modeling implementation details in the model classes. Again this demonstrates how easy it is to incorporate modeling choices into the DASE architecture. 

Firstly, we must again initialize a parameter class to feed in the corresponding Algorithm model parameters. For example, NBAlgorithm incorporates NBAlgorithmParams which holds the appropriate additive smoothing parameter lambda for MLLib's NaiveBayesModel. The main class of interest in this component is the class that extends P2LAlgorithm.
%https://docs.prediction.io/api/current/#io.prediction.controller.P2LAlgorithm
 This class must implement a method named train which will output a Training Model implementation. It must also implement a predict method that vectorizes an object and predicts using the trained model. The vectorization function is implemented by the Data Model, and the categorization (prediction) is handled mainly by the TrainingModel. Again, this demonstrates the facility with which different models can be incorporated into PredictionIO's DASE architecture.
 
At this point, we turn our attention to the TextManipulationEngine object defined in the script \tt{Engine.scala}. We see here that the engine is initialized by specifying the DataSource, Preparator, and Serving classes, as well as a Map of algorithm names to Algorithm classes. This tells the engine which algorithms to run, and, in practice, you could have as many as you would like. You simply have to implement a new Training Model and Training Algorithm pair. The results can then be combined in the Serving component of the engine. 



\end{document}