\documentclass[a4paper,12pt]{article}

\usepackage{amsfonts, amsmath, amssymb, authblk, scrextend, hyperref, enumerate,mathtools, tikz, csquotes, mathrsfs, lmodern,arydshln, xypic, bbold, graphicx}
\usepackage[mathcal]{eucal}
\usetikzlibrary{matrix}
\usepackage[lmargin=2.5 cm,rmargin=2.5cm,tmargin=3cm,bmargin=3cm]{geometry}

\allowdisplaybreaks
\everymath{\displaystyle}
\setlength\parindent{0pt}


\newcommand*\encircled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}
\newcommand*\rected[1]{\tikz[baseline=(char.base)]{
            \node[shape=rectangle,draw,inner sep=2pt] (char) {#1};}}
\newcommand{\dlim}{\underset{\longrightarrow}{\lim} \ }
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\one}{\field{1}}
\newcommand{\disp}{\displaystyle}
\newcommand{\Z}{\field{Z}}
\newcommand{\Q}{\field{Q}}
\newcommand{\F}{\field{F}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\renewcommand{\det}[1]{\text{det}\3(#1\4)}
\newcommand{\ind}{\mathbb{1}}
\newcommand{\var}{\text{Var}}
\newcommand{\val}{\text{Val}}
\newcommand{\sd}{\text{SD}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\pr}{\text{Pr}}
\renewcommand{\bf}[1]{\textbf{#1}}
\renewcommand{\it}[1]{\textit{#1}}
\renewcommand{\tt}[1]{\texttt{#1}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\mscr}[1]{\mathscr{#1}}
\newcommand{\spec}{\textbf{spec}}
\newcommand{\A}{\field{A}}
\newcommand{\liff}{\leftrightarrow}
\newcommand{\tr}[1]{\text{trace}\3(#1\4)}
\newcommand{\limf}{\lim_{n \to \infty}}
\newcommand{\3}{\left}
\newcommand{\4}{\right}
\renewcommand{\-}[1]{{}^{-#1}}
\newcommand{\up}[1]{{}^{#1}}
\newcommand{\Id}{\text{Id}}
\newcommand{\cat}[1]{\mathcal{#1}}
\newcommand{\bdiv}{\ \textbf{div} \ }
\newcommand{\bbmod}{\ \textbf{mod} \ }
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Rel}{\text{Rel}}
\newcommand{\id}[1]{\text{id}_{#1}}
\newcommand{\proj}[1]{\text{proj}_{#1}}
\newcommand{\ppmod}[1]{\ (\text{mod $#1$})}
\newcommand{\ceil}[1]{\3\lceil \text{$#1$} \4\rceil}
\newcommand{\floor}[1]{\3\lfloor \text{$#1$} \4\rfloor}
\newcommand{\power}[1]{\mathcal{P}(#1)}
\newcommand{\tri}[1]{\triangle_{#1}}
\newcommand{\inj}{\hookrightarrow}
\newcommand{\im}{\text{Im}}
\newcommand{\edel}{\epsilon-\delta}
\newcommand{\empt}{ \varnothing}
\newcommand{\inv}{^{-1}}
\newcommand{\ideal}[1]{\mathfrak{#1}}
\def\upint{\mathchoice%
    {\mkern13mu\overline{\vphantom{\intop}\mkern7mu}\mkern-20mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
    {\mkern7mu\overline{\vphantom{\intop}\mkern7mu}\mkern-14mu}%
  \int}
\def\lowint{\mkern3mu\underline{\vphantom{\intop}\mkern7mu}\mkern-10mu\int}
\begin{document}
\title{\vspace{-1.5 cm}\bf{PredictionIO: Modeling Text Data Engine}}
\author{\vspace{-2cm}}
\date{}
\maketitle


The purpose of this tutorial is to illustrate how to model text data using PredictionIO's engine platform. We will be largely mirroring the tutorial provided for Scikit's Learn package title \enquote{Working With Text Data.} However, we will be providing instructions for building the model using PredictionIO's engine platform. The advantages of using this framework include the usage of Apache Spark for distributed computation which is crucial for computation with large data sets, as well as the capacity to use a newly trained predictive model to respond to queries in real-time. 

We will assume the user is running the PredictionIO version 0.9.2, and meets all minimum computing requirements. To download PredictionIO, follow the instructions on the Getting Started tutorial.

\section*{Engine Overview}

As a user, we are charged with collecting data and importing it into an event server. The data is then read and processed by our engine via the \tt{DataSource} and \tt{Preparation} components. The \tt{Algorithm} engine component then trains a predictive model using the processed, or prepared, data. Once we have trained a model, we are ready to deploy our engine and respond to real-time queries via the \tt{Serving} component. 

We are given our observations as a set of strings $\mathcal{S}.$ In a general setting, there are two different kind of problems we can be given. In the first, each string $s$ contained in $\mathcal{S}$ is also associated to a class label $y$ taking values in $\{1, 2, ..., K\},$ where $K$ is an integer, which we can think of as a categorical, or class, label.We may proceed by learning a predictive model $f,$ a function which takes in a string $s$ and outputs a class label, using the observed data pairs $(y, s).$ Once we have learned $f$ then given a new string $s_\text{new},$ we can predict a class label $f(s_\text{new}).$ This is an example of a supervised learning problem. To exemplify the second kind of problem, suppose we are only given the set of strings $\mathcal {S},$ and are asked to group the string together into $K$ different groups based on a set of characteristics which we extract from the strings. This is an example of an unsupervised learning problem. 

In both cases, we are tasked with learning a model $f.$ However, before this we must first represent our strings $s$ in such a manner so that we may apply existing machine learning methods provided in the Spark MLLib library. We will be using the Apache OpenNLP tools library for the purpose of data representation. 

Our engine includes all DASE components, as well as a Data Model implementation which deals with representing our text data as numeric feature vectors. It also incorporates a Training Model component which is more of a conceptual framework representing a set of Scala classes that can produce a supervised (or unsupervised) learning model $f$. For our supervised training model implementation, $f$ will be a classifier, and for our unsupervised training model, an assignment of class labels for each string in $\mathcal{S}.$ The following diagram shows the how the provided Modeling Text Data Engine template functionality is structured, and the interactions between your web/app, the provided Event Server, and our Modeling Text Data Engine.


\centerline{
\xymatrixcolsep{4pc}\xymatrix{
& \\
\encircled{\bf{Event Server}} \ar[r] & \\
& \ar[ldd] \\
& \\
\encircled{\bf{Your Web/App}} \ar[ruu]_{\text{Deployment}} \ar[uuu]^{\text{Event Data}}& 
}
\rected{
\xymatrixcolsep{2pc}\xymatrix{
\encircled{Data Source}\ar[r]& \encircled{Preparator} \ar[r] \ar[d]& \encircled{Training Algorithm} \ar[d] \ar[r] & \encircled{Serving} \\
\bf{Engine} & \encircled{Data Model} \ar[ru] & \encircled{Training Model} \ar[ru]&
}}}
\vspace{0.7cm}


We begin the data collection stage by importing a Scikit Learn dataset, \tt{20newsgroups}, into PredictionIO's event server, as well as a set of English stop words that will be used in the Data Model implementation. We will delve more into the latter event data when we reach the Data Model section.

\section*{Importing Data}

We will be using Scikit Learn's datasets and text modules, as well as the PredictionIO Python SDK. First, let's get a better feel for the data: initialize your Python interpreter and type the following lines:

\begin{verbatim}
>>> from sklearn.datasets import fetch_20newsgroups
>>> twenty_train = fetch_20newsgroups(subset = 'train',
...                                     shuffle = True,
...                                     random_state = 10)
\end{verbatim}

This initializes a dictionary-like object as can be seen in the following output:

\begin{verbatim}
>>> twenty_train.keys()
dict_keys(['filenames', 'DESCR', 'target', 'data', 'target_names'])
\end{verbatim}

What we are interested in here are the target and data keys. Type in the following line to see how we will be preparing the data prior to importing it to the PredictionIO Event Server.

\begin{verbatim}
>>> [(twenty_train.target[k], twenty_train.data[k]) 
...  for k in range(len(twenty_train.data))][0]
\end{verbatim}

Our main tool from the PredictionIO Python SDK that we will use to import our data into the event server is the class \tt{EventClient} and its method \tt{create\_event}. The function that implements this in the script MyTextEngine/data/import\_eventserver.py with respect to the \tt{20newsgroups} dataset is \tt{import\_events}. As fore mentioned, we will also be importing Scikit's library of English stop words into PredictionIO's Event Server. This is locally stored as a frozen set of strings, as can be verified by typing the following into your Python interpreter:

\begin{verbatim}
>>> from sklearn.feature_extraction import text
>>> text.ENGLISH_STOP_WORDS
\end{verbatim}

Again, the class \tt{EventClient} is the SDK component that allows us import the stop word data into the PredictionIO event server. The function used to import the stop word events is \tt{import\_stopwords} which is defined in the same script. With these two functions under our belt, we are ready to begin importing data. First, make sure PredictionIO is running, you can check this by typing \tt{pio status} on your terminal. If it is not running, start it with the command \tt{pio-start-all}. Now, let's create a new application named MyTextApp by entering the command \tt{pio app new MyTextApp} in your shell. You should see some printed output which includes your newly created access key. At this point make sure you are in the template root directory, and import the data set using the following command:

\begin{verbatim}
python data/import_eventserver.py --access_key *****
\end{verbatim}

where you will replace \tt{*****} with your actual access key. If the data is successfully imported, you should see the following subsequent printed output
 
 \begin{verbatim}
 Importing data.....
 Imported 11314 events.
 Importing stop words.....
 Imported 318 stop words.
 \end{verbatim}
 
Our data is now sitting in PredictionIO's Event Server and ready to be used by our engine. The following section will get you acquainted with the different engine components that are implemented in this engine template.

\break

\section*{Engine Components}




\end{document}